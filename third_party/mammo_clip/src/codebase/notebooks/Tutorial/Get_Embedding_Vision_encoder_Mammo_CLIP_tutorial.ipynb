{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdbe867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import sys\n",
    "sys.path.append('/restricted/projectnb/batmanlab/shawn24/PhD/Mammo-CLIP/src/codebase/')\n",
    "\n",
    "from Classifiers.models.breast_clip_classifier import BreastClipClassifier\n",
    "from Datasets.dataset_utils import get_dataloader_RSNA\n",
    "from breastclip.scheduler import LinearWarmupCosineAnnealingLR\n",
    "from metrics import pfbeta_binarized, pr_auc, compute_auprc, auroc, compute_accuracy_np_array\n",
    "from utils import seed_all, AverageMeter, timeSince\n",
    "from breastclip.model.modules import load_image_encoder, LinearClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf41b63",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e0eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.tensorboard_path = '/restricted/projectnb/batmanlab/shawn24/PhD/Mammo-CLIP/log'\n",
    "        self.checkpoints = '/restricted/projectnb/batmanlab/shawn24/PhD/Mammo-CLIP/checkpoints'\n",
    "        self.output_path = '/restricted/projectnb/batmanlab/shawn24/PhD/Mammo-CLIP/out'\n",
    "        self.data_dir = '/restricted/projectnb/batmanlab/shared/Data/RSNA_Breast_Imaging/Dataset'\n",
    "        self.img_dir = 'RSNA_Cancer_Detection/train_images_png'\n",
    "        self.clip_chk_pt_path = '/restricted/projectnb/batmanlab/shawn24/PhD/Breast-CLIP/src/codebase/outputs/upmc_clip/b5_detector_period_n/checkpoints/fold_0/b5-model-best-epoch-7.tar'\n",
    "        self.csv_file = 'RSNA_Cancer_Detection/train_folds.csv'\n",
    "        self.dataset = 'RSNA'\n",
    "        self.data_frac = 1.0\n",
    "        self.arch = 'upmc_breast_clip_det_b5_period_n_ft'\n",
    "        self.label = 'cancer'\n",
    "        self.detector_threshold = 0.1\n",
    "        self.swin_encoder = 'microsoft/swin-tiny-patch4-window7-224'\n",
    "        self.pretrained_swin_encoder = 'y'\n",
    "        self.swin_model_type = 'y'\n",
    "        self.VER = '084'\n",
    "        self.epochs_warmup = 0\n",
    "        self.num_cycles = 0.5\n",
    "        self.alpha = 10\n",
    "        self.sigma = 15\n",
    "        self.p = 1.0\n",
    "        self.mean = 0.3089279\n",
    "        self.std = 0.25053555408335154\n",
    "        self.focal_alpha = 0.6\n",
    "        self.focal_gamma = 2.0\n",
    "        self.num_classes = 1\n",
    "        self.n_folds = 4\n",
    "        self.start_fold = 0\n",
    "        self.seed = 10\n",
    "        self.batch_size = 1\n",
    "        self.num_workers = 4\n",
    "        self.epochs = 9\n",
    "        self.lr = 5.0e-5\n",
    "        self.weight_decay = 1e-4\n",
    "        self.warmup_epochs = 1\n",
    "        self.img_size = [1520, 912]\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.apex = 'y'\n",
    "        self.print_freq = 5000\n",
    "        self.log_freq = 1000\n",
    "        self.running_interactive = 'n'\n",
    "        self.inference_mode = 'n'\n",
    "        self.model_type = \"Classifier\"\n",
    "        self.weighted_BCE = 'n'\n",
    "        self.balanced_dataloader = 'n'\n",
    "\n",
    "# Create an instance of the Args class\n",
    "args = Args()\n",
    "\n",
    "# Now you can use args just like you would in your script\n",
    "print(args.tensorboard_path) \n",
    "# output: /restricted/projectnb/batmanlab/shawn24/PhD/Mammo-CLIP/log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd17343",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c745a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_base_name = 'efficientnetb5'\n",
    "args.data_dir = Path(args.data_dir)\n",
    "args.df = pd.read_csv(args.data_dir / args.csv_file)\n",
    "args.df = args.df.fillna(0)\n",
    "args.cur_fold = 0\n",
    "args.train_folds = args.df[\n",
    "                (args.df['fold'] == 1) | (args.df['fold'] == 2)].reset_index(drop=True)\n",
    "args.valid_folds = args.df[args.df['fold'] == args.cur_fold].reset_index(drop=True)\n",
    "\n",
    "print(f\"train_folds shape: {args.train_folds.shape}\")\n",
    "print(f\"valid_folds shape: {args.valid_folds.shape}\")\n",
    "# output: train_folds shape: (27258, 15)\n",
    "# output: valid_folds shape: (13682, 15)\n",
    "\n",
    "ckpt = torch.load(args.clip_chk_pt_path, map_location=\"cpu\")\n",
    "args.image_encoder_type = ckpt[\"config\"][\"model\"][\"image_encoder\"][\"name\"]\n",
    "train_loader, valid_loader = get_dataloader_RSNA(args)\n",
    "print(f'train_loader: {len(train_loader)}, valid_loader: {len(valid_loader)}')\n",
    "# output: Compose([\n",
    "#   HorizontalFlip(p=0.5),\n",
    "#   VerticalFlip(p=0.5),\n",
    "#   Affine(p=0.5, interpolation=1, mask_interpolation=0, cval=0.0, mode=0, scale={'x': (0.8, 1.2), 'y': (0.8, 1.2)}, translate_percent={'x': (0.1, 0.1), 'y': (0.1, 0.1)}, translate_px=None, rotate=(20.0, 20.0), fit_output=False, shear={'x': (20.0, 20.0), 'y': (20.0, 20.0)}, cval_mask=0.0, keep_ratio=False, rotate_method='largest_box', balanced_scale=False),\n",
    "#   ElasticTransform(p=0.5, alpha=10.0, sigma=15.0, interpolation=1, border_mode=4, value=None, mask_value=None, approximate=False, same_dxdy=False),\n",
    "# ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)\n",
    "# None\n",
    "# train_loader: 3407, valid_loader: 1711"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95398371",
   "metadata": {},
   "source": [
    "## Load Mammo-CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 1\n",
    "print(ckpt[\"config\"][\"model\"][\"image_encoder\"])\n",
    "config = ckpt[\"config\"][\"model\"][\"image_encoder\"]\n",
    "image_encoder = load_image_encoder(ckpt[\"config\"][\"model\"][\"image_encoder\"])\n",
    "image_encoder_weights = {}\n",
    "for k in ckpt[\"model\"].keys():\n",
    "    if k.startswith(\"image_encoder.\"):\n",
    "        image_encoder_weights[\".\".join(k.split(\".\")[1:])] = ckpt[\"model\"][k]\n",
    "image_encoder.load_state_dict(image_encoder_weights, strict=True)\n",
    "image_encoder_type = ckpt[\"config\"][\"model\"][\"image_encoder\"][\"model_type\"]\n",
    "image_encoder = image_encoder.to(args.device)\n",
    "\n",
    "print(image_encoder_type)\n",
    "print(config[\"name\"].lower()) \n",
    "# cnn\n",
    "# tf_efficientnet_b5_ns-detect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd22e1d",
   "metadata": {},
   "source": [
    "## Loop thorough the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_iter = tqdm(enumerate(valid_loader), desc=f\"[tutorial]\",\n",
    "                     total=len(valid_loader))\n",
    "for step, data in progress_iter:\n",
    "    inputs = data['x'].to(args.device)\n",
    "    inputs = inputs.squeeze(1).permute(0, 3, 1, 2)\n",
    "    batch_size = inputs.size(0)\n",
    "    image_features = image_encoder(inputs)\n",
    "    print(image_features.shape)\n",
    "    break\n",
    "    # torch.Size([1, 2048])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breast_clip_rtx_6000",
   "language": "python",
   "name": "breast_clip_rtx_6000"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
